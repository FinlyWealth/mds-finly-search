{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow.parquet as pq\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(df, save_path='embeddings.npz', batch_size=100):\n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "    product_ids = []\n",
    "    \n",
    "    # Keep track of valid indices\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Batch image embedding first to determine which samples are valid\n",
    "    image_paths = [f\"../data/images/{pid}.jpeg\" for pid in df['Pid'].tolist()]\n",
    "    total_image_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_num, i in enumerate(range(0, len(image_paths), batch_size), 1):\n",
    "        batch_images = []\n",
    "        batch_valid_indices = []\n",
    "        \n",
    "        for idx, path in enumerate(image_paths[i:i+batch_size]):\n",
    "            try:\n",
    "                # Open and convert image to RGB\n",
    "                image = Image.open(path).convert(\"RGB\")\n",
    "                batch_images.append(image)\n",
    "                batch_valid_indices.append(i + idx)  # Store the global index\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping problematic image {path}: {e}\")\n",
    "        \n",
    "        if batch_images:\n",
    "            try:\n",
    "                # Process images using the CLIP processor\n",
    "                inputs = processor(\n",
    "                    images=batch_images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_features = model.get_image_features(**inputs)\n",
    "                    batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                image_embeddings.extend(batch_features.cpu().numpy())\n",
    "                valid_indices.extend(batch_valid_indices)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_num}: {e}\")\n",
    "                # Skip the problematic batch\n",
    "                continue\n",
    "                \n",
    "        print(f\"\\rImage embedding batch {batch_num}/{total_image_batches} processed\", end='', flush=True)\n",
    "    \n",
    "    print(f\"\\nProcessed {len(valid_indices)} valid images out of {len(image_paths)} total images\")\n",
    "    \n",
    "    # Now process text only for valid indices\n",
    "    texts = df['Name'].iloc[valid_indices].tolist()\n",
    "    ids = df['Pid'].iloc[valid_indices].tolist()\n",
    "    \n",
    "    # Filter out blank or NaN texts\n",
    "    filtered_texts = []\n",
    "    filtered_ids = []\n",
    "    for text, pid in zip(texts, ids):\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            filtered_texts.append(text)\n",
    "            filtered_ids.append(pid)\n",
    "        else:\n",
    "            print(f\"Skipping empty or invalid text for pid {pid}\")\n",
    "    \n",
    "    # Proceed with text embedding only for filtered inputs\n",
    "    total_text_batches = (len(filtered_texts) + batch_size - 1) // batch_size\n",
    "    for batch_num, i in enumerate(range(0, len(filtered_texts), batch_size), 1):\n",
    "        batch_texts = filtered_texts[i:i+batch_size]\n",
    "        inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_features = model.get_text_features(**inputs)\n",
    "            batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "        text_embeddings.extend(batch_features.cpu().numpy())\n",
    "        product_ids.extend(filtered_ids[i:i+batch_size])\n",
    "        print(f\"\\rText embedding batch {batch_num}/{total_text_batches} processed\", end='', flush=True)\n",
    "    \n",
    "    # Save\n",
    "    np.savez(save_path, \n",
    "             text_embeddings=np.array(text_embeddings),\n",
    "             image_embeddings=np.array(image_embeddings),\n",
    "             product_ids=np.array(product_ids))\n",
    "    \n",
    "    return text_embeddings, image_embeddings, product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_product_images(df, output_zip_path=\"product_images.zip\"):\n",
    "    \"\"\"\n",
    "    Creates a zip file containing all product images that exist in the data/images directory.\n",
    "    Returns a filtered DataFrame containing only rows where images exist.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the 'Pid' column\n",
    "        output_zip_path (str): Path where the zip file should be saved\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame containing only rows where images exist\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Get the list of Pids from the DataFrame\n",
    "        pids = df['Pid'].tolist()\n",
    "        \n",
    "        # Track which Pids have images\n",
    "        valid_pids = set()\n",
    "        \n",
    "        # Copy existing images to temp directory\n",
    "        for pid in pids:\n",
    "            src_path = f\"../data/images/{pid}.jpeg\"\n",
    "            if os.path.exists(src_path):\n",
    "                dst_path = os.path.join(temp_dir, f\"{pid}.jpeg\")\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                valid_pids.add(pid)\n",
    "        \n",
    "        print(f\"Found {len(valid_pids)} existing images out of {len(pids)} Pids\")\n",
    "        \n",
    "        # Create zip file\n",
    "        shutil.make_archive(\n",
    "            output_zip_path.replace('.zip', ''),  # Remove .zip as make_archive adds it\n",
    "            'zip',\n",
    "            temp_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"Created zip file: {output_zip_path}\")\n",
    "        \n",
    "        # Return filtered DataFrame\n",
    "        filtered_df = df[df['Pid'].isin(valid_pids)].copy()\n",
    "        return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/clean/filtered_data_100k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 96447 existing images out of 100000 Pids\n",
      "Created zip file: product_images.zip\n"
     ]
    }
   ],
   "source": [
    "filtered_df = zip_product_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding batch 452/965 processed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The channel dimension is ambiguous. Got image shape (1, 128, 3). Assuming channels are the first dimension.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 453: mean must have 1 elements if it is an iterable, got 3\n",
      "Image embedding batch 520/965 processed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The channel dimension is ambiguous. Got image shape (1, 128, 3). Assuming channels are the first dimension.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing batch 521: mean must have 1 elements if it is an iterable, got 3\n",
      "Image embedding batch 965/965 processed\n",
      "Processed 96283 valid images out of 96483 total images\n",
      "Text embedding batch 963/963 processed"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Calculate embeddings\n",
    "text_embeddings, image_embeddings, product_ids = calculate_embeddings(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
