{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang/miniforge3/envs/finly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm.auto import tqdm\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embeddings(df, save_path='embeddings.npz', batch_size=100):\n",
    "    text_embeddings = []\n",
    "    image_embeddings = []\n",
    "    product_ids = []\n",
    "    \n",
    "    # Keep track of valid indices\n",
    "    valid_indices = []\n",
    "    \n",
    "    # Batch image embedding first to determine which samples are valid\n",
    "    image_paths = [f\"../data/images/{pid}.jpeg\" for pid in df['Pid'].tolist()]\n",
    "    total_image_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_num, i in enumerate(range(0, len(image_paths), batch_size), 1):\n",
    "        batch_images = []\n",
    "        batch_valid_indices = []\n",
    "        \n",
    "        for idx, path in enumerate(image_paths[i:i+batch_size]):\n",
    "            try:\n",
    "                # Open and convert image to RGB\n",
    "                image = Image.open(path).convert(\"RGB\")\n",
    "                batch_images.append(image)\n",
    "                batch_valid_indices.append(i + idx)  # Store the global index\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping problematic image {path}: {e}\")\n",
    "        \n",
    "        if batch_images:\n",
    "            try:\n",
    "                # Process images using the CLIP processor\n",
    "                inputs = processor(\n",
    "                    images=batch_images,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                ).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_features = model.get_image_features(**inputs)\n",
    "                    batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                image_embeddings.extend(batch_features.cpu().numpy())\n",
    "                valid_indices.extend(batch_valid_indices)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch {batch_num}: {e}\")\n",
    "                # Skip the problematic batch\n",
    "                continue\n",
    "                \n",
    "        print(f\"\\rImage embedding batch {batch_num}/{total_image_batches} processed\", end='', flush=True)\n",
    "    \n",
    "    print(f\"\\nProcessed {len(valid_indices)} valid images out of {len(image_paths)} total images\")\n",
    "    \n",
    "    # Now process text only for valid indices\n",
    "    texts = df['Name'].iloc[valid_indices].tolist()\n",
    "    ids = df['Pid'].iloc[valid_indices].tolist()\n",
    "    \n",
    "    total_text_batches = (len(texts) + batch_size - 1) // batch_size\n",
    "    for batch_num, i in enumerate(range(0, len(texts), batch_size), 1):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = processor(text=batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_features = model.get_text_features(**inputs)\n",
    "            batch_features /= batch_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "        text_embeddings.extend(batch_features.cpu().numpy())\n",
    "        product_ids.extend(ids[i:i+batch_size])\n",
    "        print(f\"\\rText embedding batch {batch_num}/{total_text_batches} processed\", end='', flush=True)\n",
    "    \n",
    "    print(f\"\\nFinal dataset size: {len(text_embeddings)} pairs\")\n",
    "    \n",
    "    # Save\n",
    "    np.savez(save_path, \n",
    "             text_embeddings=np.array(text_embeddings),\n",
    "             image_embeddings=np.array(image_embeddings),\n",
    "             product_ids=np.array(product_ids))\n",
    "    \n",
    "    return text_embeddings, image_embeddings, product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_description(row, column_names):\n",
    "    \"\"\"\n",
    "    Combines values from specified columns of a Pandas DataFrame row into a descriptive sentence,\n",
    "    excluding columns with NaN values for that row.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A single row from a Pandas DataFrame.\n",
    "        column_names (list): A list of column names to include in the description.  The order\n",
    "            of names in this list determines the order they appear in the sentence.\n",
    "\n",
    "    Returns:\n",
    "        str: A string containing the combined text description, or an empty string if all\n",
    "            specified columns are NaN.\n",
    "    \"\"\"\n",
    "    description = \"\"\n",
    "    valid_columns = []\n",
    "\n",
    "    for col in column_names:\n",
    "        if pd.notna(row[col]):\n",
    "            valid_columns.append(col)\n",
    "\n",
    "    if not valid_columns:\n",
    "        return \"\"  # Return empty string if all columns are NaN\n",
    "\n",
    "    # Construct the description based on available data.\n",
    "    if \"Name\" in valid_columns:\n",
    "        description += f\"The product name is {row['Name']}\"\n",
    "        if \"Color\" in valid_columns and \"Brand\" in valid_columns:\n",
    "            description += f\" with color {row['Color']} from brand {row['Brand']}\"\n",
    "        elif \"Color\" in valid_columns:\n",
    "            description += f\" with color {row['Color']}\"\n",
    "        elif \"Brand\" in valid_columns:\n",
    "            description += f\" from brand {row['Brand']}\"\n",
    "\n",
    "    if \"Price\" in valid_columns and \"PriceCurrency\" in valid_columns:\n",
    "        description += f\". The price is {row['Price']} {row['PriceCurrency']}\"\n",
    "\n",
    "    if \"Gender\" in valid_columns:\n",
    "        description += f\". It is designed for {row['Gender']}\"\n",
    "\n",
    "    if \"Size\" in valid_columns:\n",
    "        description += f\". The size is {row['Size']}\"\n",
    "    return description\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_product_images(df, output_zip_path=\"product_images.zip\"):\n",
    "    \"\"\"\n",
    "    Creates a zip file containing all product images that exist in the data/images directory.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing the 'Pid' column\n",
    "        output_zip_path (str): Path where the zip file should be saved\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        # Get the list of Pids from the DataFrame\n",
    "        pids = df['Pid'].tolist()\n",
    "        \n",
    "        # Counter for existing images\n",
    "        existing_count = 0\n",
    "        \n",
    "        # Copy existing images to temp directory\n",
    "        for pid in pids:\n",
    "            src_path = f\"../data/images/{pid}.jpeg\"\n",
    "            if os.path.exists(src_path):\n",
    "                dst_path = os.path.join(temp_dir, f\"{pid}.jpeg\")\n",
    "                shutil.copy2(src_path, dst_path)\n",
    "                existing_count += 1\n",
    "        \n",
    "        print(f\"Found {existing_count} existing images out of {len(pids)} Pids\")\n",
    "        \n",
    "        # Create zip file\n",
    "        shutil.make_archive(\n",
    "            output_zip_path.replace('.zip', ''),  # Remove .zip as make_archive adds it\n",
    "            'zip',\n",
    "            temp_dir\n",
    "        )\n",
    "        \n",
    "        print(f\"Created zip file: {output_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n",
      "Loaded 100000 rows of data\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")\n",
    "\n",
    "SAMPLE_SIZE = 100000\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(\"../data/filtered_data.parquet\", engine=\"pyarrow\")\n",
    "# Randomly sample 100k rows\n",
    "df = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "print(f\"Loaded {len(df)} rows of data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['Name', 'Price', 'PriceCurrency', 'MergedBrand', 'Color', 'Gender', 'Size']\n",
    "df[\"combined\"] = df.apply(lambda row: create_description(row, column_names), axis=1)\n",
    "df.to_csv('samples.csv')\n",
    "zip_product_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_product_images(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_id = \"openai/clip-vit-base-patch32\"\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Calculate embeddings\n",
    "text_embeddings, image_embeddings, product_ids = calculate_embeddings(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
