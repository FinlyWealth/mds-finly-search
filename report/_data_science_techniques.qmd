#### Data Source and Description

The dataset comprises two primary sources: product images and metadata. The image folder contains approximately 15 million, 100x100 pixel JPEG images. It is assumed each product will have a single corresponding image. The metadata file contains 30 columns covering product attributes such as product id, name, description, color, brand and price. FinlyWealth has obtained the dataset from their affiliate network. 

**Missing Values**

Certain columns have missing values in more than 90% of the data, such as “ShortDescription” and “Keywords”. These are unlikely to be used for our data pipeline. 
Other fields such as “Gender”, “Color”, and “Brand” also have a significant proportion of missing values. These are expected since these attributes do not apply to some products such as books. 

**High Cardinality**

Columns such as “Category”, “Color”, “Size”, and “Brand” have high cardinality as shown in @fig-uniquecounts. In addition, columns "Brand" and "Manufacturer" have a 82% overlap in their unique values so the two columns have been merged into "MergedBrand" to eliminate duplication. 

![Unique Counts per Column Diagram](../img/uniquecounts.png){#fig-uniquecounts}

**Overlap Between Product Name and Metadata**

To better understand what users are likely to search for, we analyzed the top 50 most frequent words in product names as shown in @fig-topwords. Common terms like "womens", "mens", "size", and "black" appear frequently and often overlap with metadata fields such as gender and color.

![Top 50 words in product name Diagram](../img/topwords.png){#fig-topwords}

### Proposed Approach & Methodology

We are developing a multimodal, retrieval-augmented product search system that integrates advanced natural language processing techniques. This system will combine OpenAI CLIP[@li2021supervision] (Contrastive Language-Image Pre-training), TF-IDF[@aizawa2003information] (Term Frequency–Inverse Document Frequency), and FAISS[@faiss] (Facebook AI Similarity Search) to create joint image-text embeddings, enabling seamless cross-modal search. 

At the core of the system, both textual and visual representations of product data are embedded and stored in a searchable database. When a user submits a query—whether as text, image, or both—the system generates a corresponding embedding and compares it to stored representations. The most similar matches are identified and the top k results (where k is the number of desired outputs) are retrieved and presented to the user.

![Workflow Diagram](../img/workflow.png)

#### Models and Algorithms

To support multimodal product search, we use a combination of lexical and semantic retrieval methods:

- **OpenAI CLIP** embeds both images and text into a shared vector space, enabling cross-modal search based on semantic similarity.
- **TF-IDF** provides fast, interpretable keyword-based retrieval, useful for exact matches in structured product data.
- **FAISS** enables efficient approximate nearest neighbor search over CLIP embeddings, allowing us to scale to millions of products.

#### Hybrid Retrieval Strategy

We combine results from CLIP and TF-IDF using a weighted scoring system:

- Each model returns a relevance score for a user query.
- Scores are weighted (e.g., 70% CLIP, 30% TF-IDF) and merged.
- The final ranked list balances semantic relevance with textual precision.

This hybrid approach improves retrieval accuracy across varied query types and product data quality.

#### Data Preprocessing and Feature Engineering Plans

To prepare over 15 million product listings for multimodal retrieval, we apply thorough data preprocessing and feature engineering to ensure clean, semantically rich inputs. For text data, we normalize high cardinality columns (e.g., brand, category) through case folding, frequency filtering, and removing multilingual entries based on client requirements. To create robust embeddings, we combine key fields like title, brand, and description into a unified text template. 

#### Baseline Approach: TF-IDF

As a baseline for evaluating the effectiveness of our multimodal retrieval system, we will implement a TF-IDF based retrieval model. TF-IDF is a well-established, interpretable, and lightweight method that ranks documents by the importance of query terms, making it particularly effective in structured domains like e-commerce, where product listings often contain specific terms such as brand names, sizes, colors, and categories.

While TF-IDF does not capture semantic meaning like deep learning models such as CLIP, it remains highly effective for exact keyword matching and serves as a strong, interpretable baseline for assessing retrieval performance.

#### Modeling Pipeline and Architecture

The modeling pipeline starts with preprocessing raw product data—such as names, brands, and categories—by cleaning, normalizing, and combining relevant fields into a structured text format. Images are resized to match CLIP’s input requirements. We then extract features using two approaches: TF-IDF for sparse, interpretable text representations, and CLIP for dense multimodal embeddings that align text and images in a shared semantic space. These features are indexed using inverted indexing (for TF-IDF) and FAISS (for CLIP) to support fast, scalable retrieval. At query time, user input is encoded by both models, and results are retrieved from their respective indexes. A weighted scoring system merges these results, balancing lexical precision from TF-IDF with semantic relevance from CLIP, to produce a robust and accurate set of top-ranked results.

```         
User Query
   │
   ├──► TF-IDF Tokenizer ─────────────┐
   │                                  │
   ├──► CLIP Encoder ───────┐         │
   │                        │         ▼
   ▼                        ▼     TF-IDF Index (Sparse)
CLIP Embedding        TF-IDF Vector         │
   │                        │               ▼
   ▼                        └─────► Retrieve Matches
FAISS Index (Dense)                     ▲
   │                                    │
   └───────────────► Merge & Weight Results ◄──────┐
                                       │          │
                                       ▼          │
                                 Top-K Ranked Results
```

#### Tools and Libraries

A variety of tools and libraries are used in this project to support data processing, model serving, image handling, and vector search. These tools include Numpy[@harris2020array], Flask[@flask], Pillow[@pillow], Spacy[@spacy], Huggingface[@huggingface] etc. The table below outlines each tool and its role within the system.

| Library       | Purpose in Project                                                                 |
|---------------|--------------------------------------------------------------------------------------|
| NumPy         | Efficient numerical operations, especially for vector manipulation and math ops.    |
| Flask         | Lightweight web framework used for rapid prototyping of API endpoints.              |
| FAISS         | Approximate nearest neighbor search for CLIP embeddings; enables fast vector search.|
| Hugging Face  | Access to pretrained models like CLIP; used for text and image embedding.           |
| Pillow        | Image processing library used for resizing, normalization, and format conversion.   |
| spaCy         | Natural language processing toolkit for tokenization, NER, and text normalization.  |
| Pinecone      | Scalable, cloud-based vector database for fast and persistent similarity search.    |
| PostgreSQL    | Relational database to store Embeddings. Allows for multiple columns to have ebeddings|
