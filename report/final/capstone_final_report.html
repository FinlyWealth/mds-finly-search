<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jenson Chang, Chukwunonso Ebele-Muolokwu, Catherine Meng, Jingyuan Wang">

<title>Find me a better product!</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#executive-summary" id="toc-executive-summary" class="nav-link active" data-scroll-target="#executive-summary">Executive Summary</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span class="header-section-number">1</span> Introduction</a></li>
  <li><a href="#data-science-methods" id="toc-data-science-methods" class="nav-link" data-scroll-target="#data-science-methods"><span class="header-section-number">2</span> Data Science Methods</a></li>
  <li><a href="#data-product-and-results" id="toc-data-product-and-results" class="nav-link" data-scroll-target="#data-product-and-results"><span class="header-section-number">3</span> Data Product and Results</a></li>
  <li><a href="#conclusion-and-recommendations" id="toc-conclusion-and-recommendations" class="nav-link" data-scroll-target="#conclusion-and-recommendations"><span class="header-section-number">4</span> Conclusion and Recommendations</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="capstone_final_report.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Find me a better product!</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Jenson Chang, Chukwunonso Ebele-Muolokwu, Catherine Meng, Jingyuan Wang </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="executive-summary" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="executive-summary">Executive Summary</h2>
<p><a href="https://finlywealth.com/">FinlyWealth</a> is an affiliate marketing platform that rewards customers for applying for financial products. It is now looking to expand its business by offering e-commerce products through its platform. To support this transition, a team of <a href="https://masterdatascience.ubc.ca/">Master of Data Science</a> students from the <a href="https://www.ubc.ca/">University of British Columbia</a> has developed a fast and scalable multimodal search engine that allows users to search using text, images, or both, to find the most relevant products. The final product delivers product discovery by leveraging semantic understanding, enabling more accurate and relevant search results beyond simple keyword matching.</p>
<p>Our retrieval pipeline combines multimodal CLIP <span class="citation" data-cites="openaiclip">(<a href="#ref-openaiclip" role="doc-biblioref">Radford et al. 2021</a>)</span> embeddings with text-only MiniLM <span class="citation" data-cites="huggingfaceMinilm">(<a href="#ref-huggingfaceMinilm" role="doc-biblioref">Face 2024</a>)</span> embeddings, indexing them using FAISS <span class="citation" data-cites="faiss">(<a href="#ref-faiss" role="doc-biblioref">Johnson, Douze, and Jégou 2017</a>)</span> for efficient large-scale similarity search. At query time, the system identifies semantically relevant products by retrieving similar items from the index and then applies an LLM-based <span class="citation" data-cites="openai2023gpt35">(<a href="#ref-openai2023gpt35" role="doc-biblioref">OpenAI 2023</a>)</span> reranking module to refine the ranking. The architecture consists of a Streamlit <span class="citation" data-cites="streamlit">(<a href="#ref-streamlit" role="doc-biblioref">Streamlit Inc. 2019</a>)</span> frontend, a Flask-based <span class="citation" data-cites="flask">(<a href="#ref-flask" role="doc-biblioref">Ronacher 2010</a>)</span> API backend, and a vector database<span class="citation" data-cites="pgvector">(<a href="#ref-pgvector" role="doc-biblioref">Kane 2021</a>)</span> that supports embedding-based retrieval. The system effectively handles complex natural language and multimodal queries, a common challenge in e-commerce search. Quantitatively, we observed a Recall@20 of 0.56, Precision@20 of 0.64, and an average query time of 4.24 seconds over a dataset of one million products. Our data product provides a reproducible pipeline that allows FinlyWealth to index new items, evaluate system performance, and support semantic search for future e-commerce offerings.</p>
</section>
<section id="introduction" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1</span> Introduction</h2>
<p>As FinlyWealth expands its offerings from personal finance into the e-commerce sector, it faces the challenge of delivering a scalable and effective product search experience across a rapidly growing and diverse catalog. To address this, a team of Master of Data Science students at the University of British Columbia is developing a machine learning-powered multimodal search engine that understands the semantic meaning of user queries, handling both text and image inputs to help users find relevant products more intuitively and efficiently.</p>
<p>Search in the e-commerce domain presents unique challenges due to the wide variety of ways users express their search intent. Traditional approaches, such as TF-IDF-based text search, work well for simple queries like “iPhone” or “laptop.” However, most user queries are free-form, complex, and infrequent. The existing system relies on basic keyword matching, lacks semantic understanding, struggles with spelling mistakes, and does not support multimodal inputs or large-scale performance evaluation.</p>
<section id="objective" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="objective"><span class="header-section-number">1.1</span> Objective</h3>
<p>To address these gaps, this project designed and implemented a fast, scalable multimodal search system that captures semantic meaning of user queries and returns the most relevant products to the users. Architecture components include:</p>
<div id="tbl-object" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-object-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Summary of Client Requirements and Our Solutions
</figcaption>
<div aria-describedby="tbl-object-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 35%">
<col style="width: 64%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Client Requirement</strong></th>
<th><strong>Our Solution</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Support for natural language and multimodal queries</td>
<td>Combined CLIP (image-text) and MiniLM (text-only) embeddings; LLM-based reranking for semantic relevance</td>
</tr>
<tr class="even">
<td>Fast response time</td>
<td>Indexed embeddings using FAISS for efficient approximate nearest neighbor search</td>
</tr>
<tr class="odd">
<td>Reusable API endpoints</td>
<td>Developed modular backend with Flask APIs</td>
</tr>
<tr class="even">
<td>Reproducible data pipeline</td>
<td>Designed modular indexing, query search, and evaluation pipelines, automated via <code>make</code></td>
</tr>
<tr class="odd">
<td>Web interface for user interaction</td>
<td>Built a user-friendly interface using Streamlit</td>
</tr>
<tr class="even">
<td>Transparent evaluation and benchmarking</td>
<td>Proposed evaluation plan: Recall@20, Precision@20 (human-judged), and query time</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>To support scalable data storage, we use PostgreSQL with the pgvector extension, providing an affordable and efficient solution for storing embeddings and associated metadata.</p>
<p>The final data product is evaluated using the following evaluation metrics:</p>
<ul>
<li><p>Recall@K: Measures how often the intended or relevant product appears in the top K retrieved results</p></li>
<li><p>Precision@K: Measures how many of the top K retrieved products are actually relevant, based on manual human relevance assessments</p></li>
<li><p>Query time: Measures how long each query takes to return results (target &lt;= 5 seconds)</p></li>
</ul>
</section>
</section>
<section id="data-science-methods" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="data-science-methods"><span class="header-section-number">2</span> Data Science Methods</h2>
<section id="data-source-description-and-cleaning" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="data-source-description-and-cleaning"><span class="header-section-number">2.1</span> Data Source, Description and Cleaning</h3>
<p>The dataset consists of multimodal product data, including images (14,684,588 JPEG files, approximately 67 GB), textual information (product names and descriptions), and structured metadata (e.g., <code>Category</code>, <code>Brand</code>, <code>Color</code>). The metadata is stored in a 12 GB CSV file containing 15,384,100 rows and 30 columns.</p>
<p>After conducting exploratory data analysis and consulting with our partner, we selected the 16 most relevant columns that capture the key information users care about. We excluded non-English market entries—retaining approximately 70% of the dataset—in line with our partner’s business focus. Additionally, we merged the <code>Brand</code> and <code>Manufacturer</code> columns into a single <code>MergedBrand</code> field to reduce duplication while preserving distinct brand information. We chose to ignore missing values in the metadata columns, as these fields are likely to provide supplementary information, while the product name already contains the primary details (<a href="#tbl-keptcolumns" class="quarto-xref">Table&nbsp;2</a>).</p>
<div id="tbl-keptcolumns" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-keptcolumns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;2: Summary of Retained Columns and Their Characteristics
</figcaption>
<div aria-describedby="tbl-keptcolumns-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 17%">
<col style="width: 57%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Group</strong></th>
<th><strong>Attribute</strong></th>
<th><strong>Description / Examples</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Identifiers</strong></td>
<td><code>Pid</code></td>
<td>Unique product ID; links to image filenames</td>
</tr>
<tr class="even">
<td><strong>Text Fields</strong></td>
<td><code>Name</code></td>
<td>Product title (0.2% missing)</td>
</tr>
<tr class="odd">
<td></td>
<td><code>Description</code></td>
<td>Product description (0.03% missing)</td>
</tr>
<tr class="even">
<td></td>
<td><code>Category</code></td>
<td>Product category (28% missing; ~15 K unique values)</td>
</tr>
<tr class="odd">
<td><strong>Pricing &amp; Availability</strong></td>
<td><code>Price</code></td>
<td>Listed price</td>
</tr>
<tr class="even">
<td></td>
<td><code>"PriceCurrency"</code></td>
<td>Currency of the price</td>
</tr>
<tr class="odd">
<td></td>
<td><code>FinalPrice</code></td>
<td>Final price after discounts</td>
</tr>
<tr class="even">
<td></td>
<td><code>Discount</code></td>
<td>Discount percentage or value</td>
</tr>
<tr class="odd">
<td></td>
<td><code>isOnSale</code></td>
<td>Boolean flag</td>
</tr>
<tr class="even">
<td></td>
<td><code>IsInStock</code></td>
<td>Boolean flag</td>
</tr>
<tr class="odd">
<td><strong>Branding</strong></td>
<td><code>Brand</code></td>
<td>Brand name (53% missing; ~21 K unique values)</td>
</tr>
<tr class="even">
<td></td>
<td><code>Manufacturer</code></td>
<td>Manufacturer name (34% missing; ~26 K unique values)</td>
</tr>
<tr class="odd">
<td><strong>Product Features</strong></td>
<td><code>Color</code></td>
<td>Product color (49% missing; ~170 K unique values)</td>
</tr>
<tr class="even">
<td></td>
<td><code>Gender</code></td>
<td>Target gender (54% missing; 3 values: e.g., male/female)</td>
</tr>
<tr class="odd">
<td></td>
<td><code>Size</code></td>
<td>Product size (46% missing; ~55 K unique values)</td>
</tr>
<tr class="even">
<td></td>
<td><code>Condition</code></td>
<td>Product condition (e.g., new, used; 5 values)</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
<p>Given the timeline for this project, we’ve selected 1M dataset out of the 15M to build the final data product.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="indexing-pipeline" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="indexing-pipeline"><span class="header-section-number">2.2</span> Indexing Pipeline</h3>
<p>Our goal was to develop a multimodal search engine capable of delivering relevant product results for a wide range of customer queries. To support this, we designed a system that encodes product data with both text and image understanding and enables scalable retrieval of similar items. The system incorporates TF-IDF for keyword-based matching, CLIP for aligning visual and textual information, MiniLM for efficient semantic text encoding, and FAISS for scalable vector similarity search. This pipeline (<a href="#fig-index-pipeline" class="quarto-xref">Figure&nbsp;1</a>) is then used to convert the 1M product data into indices that can be searched.</p>
<div id="fig-index-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-index-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../img/index_pipeline.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-index-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Indexing Product Data
</figcaption>
</figure>
</div>
<section id="cleanind-data" class="level4" data-number="2.2.1">
<h4 data-number="2.2.1" class="anchored" data-anchor-id="cleanind-data"><span class="header-section-number">2.2.1</span> Cleanind Data</h4>
<p>The dataset was filtered to include only products priced in <em>USD</em>, <em>CAD</em>, or <em>GBP</em>, ensuring that associated metadata—such as product descriptions—is predominantly in English. Additionally, the <code>Brand</code> and <code>Manufacturer</code> fields, which contained largely redundant information, were consolidated into a single column to reduce duplication and improve consistency.</p>
</section>
<section id="generating-embeddings" class="level4" data-number="2.2.2">
<h4 data-number="2.2.2" class="anchored" data-anchor-id="generating-embeddings"><span class="header-section-number">2.2.2</span> Generating Embeddings</h4>
<p>Our embedding strategy was inspired by Liu and Lopez Ramos <span class="citation" data-cites="liu2025multimodal">(<a href="#ref-liu2025multimodal" role="doc-biblioref">Liu and Lopez Ramos 2025</a>)</span>, who combined CLIP and a BERT model fine-tuned on e-commerce data to enhance product search relevance. Since we lacked access to labeled, domain-specific data for fine-tuning, we opted for MiniLM <span class="citation" data-cites="huggingfaceMinilm">(<a href="#ref-huggingfaceMinilm" role="doc-biblioref">Face 2024</a>)</span>—a smaller, faster transformer model that performs well out-of-the-box and provides solid semantic understanding. We generate embeddings using both CLIP (for image-text alignment) and MiniLM (for textual metadata), then concatenate them into a single unified embedding, which is stored in a vector database for retrieval.</p>
</section>
<section id="clustering-generated-embeddings" class="level4" data-number="2.2.3">
<h4 data-number="2.2.3" class="anchored" data-anchor-id="clustering-generated-embeddings"><span class="header-section-number">2.2.3</span> Clustering Generated Embeddings</h4>
<p>To support scalable and efficient retrieval, we leveraged FAISS, a library optimized for fast similarity search and clustering of dense vectors. We tuned key hyperparameters to determine the optimal number of clusters (nlist) and the number of clusters to probe during search (nprobe). We selected 10,000 clusters, as it provided similar best performance as shown in <a href="#fig-faiss-hyperparam" class="quarto-xref">Figure&nbsp;2</a>. During retrieval, we search across the top 32 clusters, striking a balance between speed and recall. Using an Inverted File Index (IVF), we clustered 1 million products into 10,000 groups, with each product assigned to its nearest centroid. At query time, FAISS limits the search to the most relevant clusters, significantly improving search efficiency over exhaustive approaches.</p>
<div id="fig-faiss-hyperparam" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-faiss-hyperparam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../img/faiss_hyperparam.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-faiss-hyperparam-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Hyperparameter Seach for FAISS Cluster Size
</figcaption>
</figure>
</div>
</section>
<section id="processing-metadata-text" class="level4" data-number="2.2.4">
<h4 data-number="2.2.4" class="anchored" data-anchor-id="processing-metadata-text"><span class="header-section-number">2.2.4</span> Processing Metadata Text</h4>
<p>In addition to vector-based methods, we implemented a traditional keyword-based search using TF-IDF, which ranks products based on the relevance to the query. Product descriptions and attributes are processed into <em>tsvector</em> format and stored in a PostgreSQL database. A <em>tsvector</em> is a specialized data type for full-text search in Postgres that tokenizes text into lexemes (root word forms) and removes stopwords, enabling fast and accurate query matching through the <em>tsquery</em> syntax <span class="citation" data-cites="postgres-textsearch">(<a href="#ref-postgres-textsearch" role="doc-biblioref">PostgreSQL Global Development Group, n.d.</a>)</span>.</p>
</section>
</section>
<section id="search-pipeline" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="search-pipeline"><span class="header-section-number">2.3</span> Search Pipeline</h3>
<section id="generating-query-embeddings" class="level4" data-number="2.3.1">
<h4 data-number="2.3.1" class="anchored" data-anchor-id="generating-query-embeddings"><span class="header-section-number">2.3.1</span> Generating Query Embeddings</h4>
<p>When a search query is submitted, we process it in two forms: the raw text and its corresponding embedding. The raw text is used for traditional full-text search, while the embedding is used for vector-based retrieval. Each method returns a ranked list of results, which are then combined using a weighted scoring system. To further enhance relevance, we apply a Large Language Model (LLM) to rerank the top results based on deeper semantic understanding (<a href="#fig-text-pipeline" class="quarto-xref">Figure&nbsp;3</a>).</p>
<div id="fig-text-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-text-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../img/text_pipeline.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-text-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Workflow for a Search Query using Text only or Text &amp; Image
</figcaption>
</figure>
</div>
<p>For image-only queries, the full text search and LLM reranking step is skipped since there are no text inputs to use (<a href="#fig-image-pipeline" class="quarto-xref">Figure&nbsp;4</a>).</p>
<div id="fig-image-pipeline" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-image-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../img/image_pipeline.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-pipeline-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Workflow for a Search Query using Image only
</figcaption>
</figure>
</div>
</section>
<section id="reranking-with-a-large-language-model-llm" class="level4" data-number="2.3.2">
<h4 data-number="2.3.2" class="anchored" data-anchor-id="reranking-with-a-large-language-model-llm"><span class="header-section-number">2.3.2</span> Reranking with a Large Language Model (LLM)</h4>
<p>The LLM plays a key role in improving result relevance by reranking the initial set of retrieved products. It helps interpret the user’s intent and refines the rankings based on multiple criteria, including:</p>
<ol type="1">
<li>Semantic similarity to the query intent</li>
<li>Direct keyword matches</li>
<li>Mentions of specific brand names</li>
<li>Price relevance compared to similar items</li>
</ol>
<p>Reranking is particularly important because embedding retrieval could return items that are broadly relevant but lack fine-grained alignment with the user’s actual intent. LLMs offer a more nuanced understanding of both the query and the retrieved content, enabling more accurate prioritization of results. This is particularly useful for natural language queries, where the user’s intent may be complex or not explicitly stated.</p>
<p>For example, if a user searches for “a cheap office chair for home use,” the user has not explicitly specified a price point and the initial results may include a mix of premium and budget options. An LLM can interpret “cheap” as a key signal and evaluate product prices within the context of similar items. It can lower the ranking of high-end chairs and highlight budget-friendly options that better reflect the user’s intent, which embedding retrieval might not account for.</p>
</section>
</section>
<section id="evaluation" class="level3" data-number="2.4">
<h3 data-number="2.4" class="anchored" data-anchor-id="evaluation"><span class="header-section-number">2.4</span> Evaluation</h3>
<p>This project focused on improving search performance for natural language queries, where traditional keyword-based methods often fail. We compared three configurations: Text Search (baseline), Text + Embeddings, and Text + Embeddings + LLM. The three configurations were evaluated on <strong>Recall@20</strong>, <strong>Precision@20</strong>, and <strong>Search Time</strong>. A summary of evaluation results is provided in <a href="#tbl-eval-summary" class="quarto-xref">Table&nbsp;3</a>.</p>
<div class="cell" data-execution_count="1">
<div id="tbl-eval-summary" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="1">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-eval-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3: Evaluation Summary
</figcaption>
<div aria-describedby="tbl-eval-summary-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display" data-execution_count="8">
<div>


<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Method</th>
<th data-quarto-table-cell-role="th">Query Type</th>
<th data-quarto-table-cell-role="th">Recall@20</th>
<th data-quarto-table-cell-role="th">Precision@20</th>
<th data-quarto-table-cell-role="th">Search Time (s)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Text Search</td>
<td>Basic Query</td>
<td>0.42</td>
<td>0.73</td>
<td>0.30</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>Text + Embeddings</td>
<td>Basic Query</td>
<td>0.33</td>
<td>0.81</td>
<td>0.60</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>Text + Embeddings + LLM</td>
<td>Basic Query</td>
<td>0.41</td>
<td>0.78</td>
<td>4.24</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>Text Search</td>
<td>Natural Query</td>
<td>0.07</td>
<td>0.07</td>
<td>0.30</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>Text + Embeddings</td>
<td>Natural Query</td>
<td>0.53</td>
<td>0.70</td>
<td>0.60</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>Text + Embeddings + LLM</td>
<td>Natural Query</td>
<td>0.58</td>
<td>0.62</td>
<td>4.24</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</figure>
</div>
</div>
<section id="recall" class="level4" data-number="2.4.1">
<h4 data-number="2.4.1" class="anchored" data-anchor-id="recall"><span class="header-section-number">2.4.1</span> Recall</h4>
<p>Recall@20 is calculated based on whether the specific target product being searched for appears within the top 20 retrieved results. This evaluation reflects whether the system is able to surface the exact intended product, which is particularly important for e-commerce use cases where users often look for a specific item.</p>
<p>Recall saw the most improvement for natural queries, the primary focus of this project. The baseline Text Search method retrieved only 7% of relevant results, underscoring its limitations for conversational input. By adding semantic embeddings and LLM reranking, recall was increased to 58%. This highlights the LLM’s ability to recover more relevant items beyond those matched by keywords or nearest-neighbor search.</p>
</section>
<section id="precision" class="level4" data-number="2.4.2">
<h4 data-number="2.4.2" class="anchored" data-anchor-id="precision"><span class="header-section-number">2.4.2</span> Precision</h4>
<p>Precision@20 measures the proportion of the top 20 results that are relevant to the query, based on human judgment. It reflects the ranking quality—how many of the returned products are actually useful to the user.</p>
<p>Precision also improved substantially for natural queries, rising from 7% with baseline Text Search to 70% with Text + Embeddings, and 62% with the LLM-enhanced pipeline. The slight drop in precision with the LLM is likely due to the subjective nature of our evaluation process. In the absence of labeled ground truth, relevance was manually assessed by team members using their own judgment. Without standardized annotation guidelines, this introduces variability in what is considered “relevant”.</p>
</section>
<section id="search-time" class="level4" data-number="2.4.3">
<h4 data-number="2.4.3" class="anchored" data-anchor-id="search-time"><span class="header-section-number">2.4.3</span> Search Time</h4>
<p>Search time captures the total time taken to process a query and return results. It helps evaluate the responsiveness of the system under different configurations.</p>
<p>Search time increased as more complex processing was introduced. The LLM-based reranking step added significant overhead, bringing total query time to 4.24 seconds. This additional time is due entirely to the reranking process, where the LLM semantically evaluates and reorders the top results. Overall, we are still under the target time of 5 seconds.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="data-product-and-results" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="data-product-and-results"><span class="header-section-number">3</span> Data Product and Results</h2>
<p>The data product is comprised of preprocessing scripts, a frontend interface and a backend API.</p>
<section id="indexing-pipeline-1" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="indexing-pipeline-1"><span class="header-section-number">3.1</span> Indexing Pipeline</h3>
<p>The indexing pipeline involves data cleaning, followed by embedding generation, database loading, and finally, FAISS index generation. This process is initiated via the <code>make index</code> command, which executes the aforementioned steps to prepare the products for contextual querying. The pipeline executes its steps in the following sequential order:</p>
<ul>
<li><code>clean_data.py</code>: Cleans the raw CSV data by removing null values, filtering for English products etc.</li>
<li><code>generate_embed.py</code>: Generates embeddings from product names using MiniLM <span class="citation" data-cites="Wang2020MiniLMDS">(<a href="#ref-Wang2020MiniLMDS" role="doc-biblioref">Wang et al. 2020</a>)</span> and from images using CLIP <span class="citation" data-cites="openaiclip">(<a href="#ref-openaiclip" role="doc-biblioref">Radford et al. 2021</a>)</span>, respectively.</li>
<li><code>load_db.py</code>: Loads these generated embeddings and associated product metadata into the PGVector database.</li>
<li><code>compute_faiss_index.py</code> : Compute the FAISS indices for faster search.</li>
</ul>
</section>
<section id="frontend-interface" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="frontend-interface"><span class="header-section-number">3.2</span> Frontend Interface</h3>
<p>The Streamlit-based frontend serves as an internal tool for evaluating the quality of search results and testing the underlying API. It supports a range of query types—including text-only, image-only, and multimodal inputs. The interface also provide summary statistics on the retrieved results.</p>
<div id="fig-interface" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-interface-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../../img/interface.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-interface-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: User Interface
</figcaption>
</figure>
</div>
<p><strong>Key Features:</strong></p>
<ol type="1">
<li><p><strong>Multimodal Input</strong>: Supports both text queries and image uploads as seen in as [1] in <a href="#fig-interface" class="quarto-xref">Figure&nbsp;5</a></p></li>
<li><p><strong>Rich Results Display</strong>: Product cards with images, prices, and detailed metadata</p></li>
<li><p><strong>Analytics Dashboard</strong>: Live statistics on search results including price ranges, brand distribution, category breakdowns and LLM reasoning as seen as [2] in <a href="#fig-interface" class="quarto-xref">Figure&nbsp;5</a></p></li>
<li><p><strong>User Experience Design:</strong></p>
<ul>
<li>Progressive result loading (20 results initially, expandable)</li>
<li>Visual feedback for user interactions (Precision). This is the thumbs up and thumbs down button labelled as [4] as seen in <a href="#fig-interface" class="quarto-xref">Figure&nbsp;5</a>. These results are collected and are used in calculating precision metrics.</li>
</ul></li>
</ol>
</section>
<section id="backend-api" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="backend-api"><span class="header-section-number">3.3</span> Backend API</h3>
<p>The Flask-based <span class="citation" data-cites="flask">(<a href="#ref-flask" role="doc-biblioref">Ronacher 2010</a>)</span> REST API serves as the core processing engine:</p>
<p><strong>Endpoints:</strong></p>
<ul>
<li><p><code>POST /api/search</code>: Main search functionality supporting text, image, and multimodal queries</p></li>
<li><p><code>GET /api/ready</code>: Health check and initialization status</p></li>
<li><p><code>POST /api/feedback</code>: User feedback collection for continuous improvement</p></li>
</ul>
<div style="page-break-after: always;"></div>
<p><strong>Query Workflow:</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../img/search_workflow.png" class="img-fluid figure-img"></p>
<figcaption>Search Workflow</figcaption>
</figure>
</div>
<p>Our query workflow starts with passing the search query to the API. This is followed by <em>Embedding Generation</em>, which creates appropriate vector representations. Next, a <em>Hybrid Retrieval</em> step combines both vector similarity and full-text search for comprehensive results. Subsequently, <em>LLM Reranking</em>, utilizing models like OpenAI GPT, optimizes the relevance of the retrieved information. Finally, the top retrieval results are sent back to the frontend.</p>
</section>
<section id="database-and-storage" class="level3" data-number="3.4">
<h3 data-number="3.4" class="anchored" data-anchor-id="database-and-storage"><span class="header-section-number">3.4</span> Database and Storage</h3>
<p>The system’s data infrastructure is built on Google Cloud. Product metadata and embeddings are stored in a PostgreSQL database with the pgvector extension on Cloud SQL, primarily for retrieval and indexing purposes. Similarity search is performed using FAISS indices, which are stored on Google Cloud Storage alongside product images. This storage setup is highly scalable, making it easy to accommodate growing volumes of product images and embedding indices as the catalog expands.</p>
</section>
<section id="strengths-and-limitations" class="level3" data-number="3.5">
<h3 data-number="3.5" class="anchored" data-anchor-id="strengths-and-limitations"><span class="header-section-number">3.5</span> Strengths and Limitations</h3>
<p>This section outlines the core strengths and current technical constraints of the search system. While the architecture is designed for flexibility, speed, and multimodal support, certain trade-offs exist due to reliance on pre-trained models and resource requirements.</p>
<section id="key-advantages" class="level4" data-number="3.5.1">
<h4 data-number="3.5.1" class="anchored" data-anchor-id="key-advantages"><span class="header-section-number">3.5.1</span> Key Advantages</h4>
<ol type="1">
<li><strong>Multimodal Capability</strong>: Unique ability to process both text and image queries simultaneously</li>
<li><strong>Hybrid Search Architecture</strong>: Combines vector similarity with traditional full-text search for improved recall</li>
<li><strong>Scalable Design</strong>: FAISS indices enable sub-second search across millions of products</li>
<li><strong>Flexible Model Integration</strong>: Supports multiple embedding models and LLM providers</li>
</ol>
</section>
<section id="technical-constraints" class="level4" data-number="3.5.2">
<h4 data-number="3.5.2" class="anchored" data-anchor-id="technical-constraints"><span class="header-section-number">3.5.2</span> Technical Constraints</h4>
<ol type="1">
<li><strong>Model Dependencies</strong>: Relies on pre-trained models that may not be domain-specific. No training done</li>
<li><strong>Memory Requirements</strong>: Large embedding matrices require significant RAM and storage for optimal performance</li>
<li><strong>Single-Language Support</strong>: Currently optimized only for English queries</li>
<li><strong>Update Propagation</strong>: Adding new products requires recomputing embeddings and rebuilding indices</li>
</ol>
</section>
</section>
<section id="potential-improvements-and-implementation-challenges" class="level3" data-number="3.6">
<h3 data-number="3.6" class="anchored" data-anchor-id="potential-improvements-and-implementation-challenges"><span class="header-section-number">3.6</span> Potential Improvements and Implementation Challenges</h3>
<p>As the system evolves, several enhancements can be explored to boost retrieval accuracy, scalability, and user relevance. This section highlights key opportunities identified through initial experimentation and outlines the potential benefits of each, along with the practical challenges they present.</p>
<section id="advanced-keyword-extraction-with-keybert" class="level4" data-number="3.6.1">
<h4 data-number="3.6.1" class="anchored" data-anchor-id="advanced-keyword-extraction-with-keybert"><span class="header-section-number">3.6.1</span> Advanced Keyword Extraction with KeyBERT</h4>
<ul>
<li><strong>Improvement</strong>: Implement KeyBERT for automatic keyword extraction to enrich text embeddings. This was explored and improved the recall score</li>
<li><strong>Benefits</strong>: Better understanding of product attributes and user intent</li>
<li><strong>Implementation Challenge</strong>: Requires additional compute resources for keyword processing</li>
</ul>
</section>
<section id="premium-embedding-models" class="level4" data-number="3.6.2">
<h4 data-number="3.6.2" class="anchored" data-anchor-id="premium-embedding-models"><span class="header-section-number">3.6.2</span> Premium Embedding Models</h4>
<ul>
<li><strong>Improvement</strong>: Upgrade to OpenAI’s text-embedding-3-large or similar high-performance models</li>
<li><strong>Benefits</strong>: Superior semantic understanding and cross-domain generalization</li>
<li><strong>Implementation Challenge</strong>: Significantly higher API costs and embedding size</li>
</ul>
</section>
<section id="llm-prompt-engineering-with-real-customer-data" class="level4" data-number="3.6.3">
<h4 data-number="3.6.3" class="anchored" data-anchor-id="llm-prompt-engineering-with-real-customer-data"><span class="header-section-number">3.6.3</span> LLM Prompt Engineering with Real Customer Data</h4>
<ul>
<li><strong>Improvement</strong>: Develop sophisticated prompts using actual user search patterns and feedback</li>
<li><strong>Benefits</strong>: More contextually aware result reranking</li>
<li><strong>Implementation Challenge</strong>: Privacy concerns and data collection complexity</li>
</ul>
</section>
<section id="managed-vector-database-migration" class="level4" data-number="3.6.4">
<h4 data-number="3.6.4" class="anchored" data-anchor-id="managed-vector-database-migration"><span class="header-section-number">3.6.4</span> Managed Vector Database Migration</h4>
<ul>
<li><strong>Improvement</strong>: Transition to Pinecone or similar managed vector database services</li>
<li><strong>Benefits</strong>: Reduced operational overhead, better scalability, advanced features</li>
<li><strong>Implementation Challenge</strong>: Migration complexity and ongoing costs</li>
<li><strong>Cost-Benefit Analysis</strong>: Higher operational costs but reduced engineering overhead</li>
</ul>
</section>
</section>
</section>
<section id="conclusion-and-recommendations" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="conclusion-and-recommendations"><span class="header-section-number">4</span> Conclusion and Recommendations</h2>
<p>We have developed a fast and scalable multimodal search engine that allows users to retrieve relevant products using text, image, or hybrid queries. In contrast to Finly’s original platform, which relies on direct keyword matching, our system is built to understand the semantic meaning of natural language and handle complex queries effectively. Even with the most computationally intensive model, the system maintains a response time under 5 seconds, meeting the usability standards for customer-facing applications.</p>
<p>To achieve this, we integrated multiple models and tools. For multimodal capability, we leveraged CLIP <span class="citation" data-cites="openaiclip">(<a href="#ref-openaiclip" role="doc-biblioref">Radford et al. 2021</a>)</span> to extract features from both text and images. To capture semantic information, we incorporated MiniLM <span class="citation" data-cites="huggingfaceMinilm">(<a href="#ref-huggingfaceMinilm" role="doc-biblioref">Face 2024</a>)</span> along with an LLM-based reranking module <span class="citation" data-cites="openai2023gpt35">(<a href="#ref-openai2023gpt35" role="doc-biblioref">OpenAI 2023</a>)</span>. To ensure low latency, we implemented FAISS indexing <span class="citation" data-cites="faiss">(<a href="#ref-faiss" role="doc-biblioref">Johnson, Douze, and Jégou 2017</a>)</span> for efficient similarity search. Additionally, we adopted Google Cloud for data storage to meet the scalability requirements.</p>
<p>In our performance evaluation on a dataset of one million products, the system achieved a Recall@20 of 0.56, Precision@20 of 0.64, and an average search time of 4.24 seconds. These results demonstrate that our search engine is both accurate and responsive.</p>
<section id="recommendations" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="recommendations"><span class="header-section-number">4.1</span> Recommendations</h3>
<p>Our product successfully met all of Finly’s requirements. We also developed a web interface that presents a statistical summary of the retrieved results and integrated a modular evaluation framework for our partner. However, several limitations of the current system should be noted.</p>
<ul>
<li><p><strong>Evaluation limitations</strong>: The precision evaluation was based on the manual annotation of the top 20 retrieved products by our team. Given our limited domain expertise in e-commerce and the subjective interpretation of what constitutes a “relevant” product, the labeling may suffer from inconsistency and potential bias. To improve reliability, we recommend involving an e-commerce expert to standardize annotation guidelines and ensure a more professional and consistent evaluation process.</p></li>
<li><p><strong>Scalability and infrastructure</strong>: Currently, our reranking module is applied only to the top 30 retrieved products due to its relatively long execution time. We did not conduct experiments to determine the optimal cutoff threshold that balances performance and latency, primarily due to time and resource constraints. However, we believe that implementing an adaptive cutoff strategy could be a valuable direction for future enhancement.</p>
<p>Additionally, due to limited computing resources, we generated embeddings for only one million products, rather than the entire product catalog. This limitation can be easily addressed by rerunning our reproducible indexing pipeline once Finly gains access to sufficient computational infrastructure.</p></li>
<li><p><strong>Data constraints</strong>: Due to the absence of labeled customer interaction data, our current similarity search relies solely on fusion embeddings of text and image inputs, without any model fine-tuning. Prior academic research <span class="citation" data-cites="liu2025multimodal">(<a href="#ref-liu2025multimodal" role="doc-biblioref">Liu and Lopez Ramos 2025</a>)</span> suggests that adding a projection layer on top of the fusion embedding can improve performance. Once Finly acquires sufficient labeled data, the pipeline can be adapted to include such a layer along with an appropriately designed loss function.</p></li>
</ul>
<p>Despite the limitations discussed above, our solution offers FinlyWealth a robust, scalable architecture and a reproducible development pipeline. This positions the company well to scale the system further and adapt it to the growing and evolving needs of its e-commerce platform.</p>
<div style="page-break-after: always;"></div>
</section>
</section>
<section id="appendix" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="appendix">Appendix</h2>
<section id="tools-and-libraries" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="tools-and-libraries">Tools and Libraries</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Library</th>
<th>Purpose in Project</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://numpy.org/">NumPy</a></td>
<td>Efficient numerical operations, especially for vector manipulation and math ops.</td>
</tr>
<tr class="even">
<td><a href="https://flask.palletsprojects.com/en/stable/">Flask</a></td>
<td>Lightweight web framework used for rapid prototyping of API endpoints.</td>
</tr>
<tr class="odd">
<td><a href="https://github.com/facebookresearch/faiss">FAISS</a></td>
<td>Approximate nearest neighbor search for CLIP embeddings; enables fast vector search.</td>
</tr>
<tr class="even">
<td><a href="https://huggingface.co/">Hugging Face</a></td>
<td>Access to pretrained models like CLIP; used for text and image embedding.</td>
</tr>
<tr class="odd">
<td><a href="https://pillow.readthedocs.io/en/stable/">Pillow</a></td>
<td>Image processing library used for resizing, normalization, and format conversion.</td>
</tr>
<tr class="even">
<td><a href="https://spacy.io/">spaCy</a></td>
<td>Natural language processing toolkit for tokenization, NER, and text normalization.</td>
</tr>
<tr class="odd">
<td><a href="https://www.pinecone.io/">Pinecone</a></td>
<td>Scalable, cloud-based vector database for fast and persistent similarity search.</td>
</tr>
<tr class="even">
<td><a href="https://www.postgresql.org/">PostgreSQL</a></td>
<td>Relational database to store Embeddings. Allows for multiple columns to have ebeddings</td>
</tr>
</tbody>
</table>
</section>
<section id="definitions" class="level3 unnumbered">
<h3 class="unnumbered anchored" data-anchor-id="definitions">Definitions</h3>
<p><strong>CLIP:</strong> Generates embeddings for both text and images, mapping them into a shared embedding space. We are not training any embedding model, instead we use off-the-shelf <a href="https://huggingface.co/docs/transformers/en/model_doc/clip">CLIP models</a> to generate embeddings.</p>
<p><strong>Embedding Generation:</strong> The preprocessed query is then transformed into a numerical representation (an embedding) that captures its semantic meaning.</p>
<p><strong>FAISS</strong> (Facebook AI Similarity Search) is a library that allows developers to quickly search for embeddings of multimedia documents.Enables efficient approximate nearest neighbor search over embeddings.</p>
<p><strong>TF-IDF:</strong> A numerical statistic used to evaluate the importance of a word in a document within a collection of documents</p>
</section>
</section>
<section id="references" class="level2 unnumbered">




</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-huggingfaceMinilm" class="csl-entry" role="listitem">
Face, Hugging. 2024. <span>“MiniLM on Hugging Face.”</span> <a href="https://huggingface.co/models">https://huggingface.co/models</a>.
</div>
<div id="ref-faiss" class="csl-entry" role="listitem">
Johnson, Jeff, Matthijs Douze, and Hervé Jégou. 2017. <span>“<span>FAISS</span>: A Library for Efficient Similarity Search and Clustering of Dense Vectors.”</span> <a href="https://github.com/facebookresearch/faiss" class="uri">https://github.com/facebookresearch/faiss</a>.
</div>
<div id="ref-pgvector" class="csl-entry" role="listitem">
Kane, Andrew. 2021. <span>“Pgvector: Open-Source Vector Similarity Search for Postgres.”</span> <a href="https://github.com/pgvector/pgvector" class="uri">https://github.com/pgvector/pgvector</a>.
</div>
<div id="ref-liu2025multimodal" class="csl-entry" role="listitem">
Liu, Dong, and Esther Lopez Ramos. 2025. <span>“Multimodal Semantic Retrieval for Product Search.”</span> <em>arXiv Preprint arXiv:2501.07365</em>, January. <a href="https://doi.org/10.48550/arXiv.2501.07365">https://doi.org/10.48550/arXiv.2501.07365</a>.
</div>
<div id="ref-openai2023gpt35" class="csl-entry" role="listitem">
OpenAI. 2023. <span>“GPT-3.5-Turbo Model.”</span> https://platform.openai.com/docs/models/gpt-3-5.
</div>
<div id="ref-postgres-textsearch" class="csl-entry" role="listitem">
PostgreSQL Global Development Group. n.d. <span>“Text Search Types.”</span> <a href="https://www.postgresql.org/docs/current/datatype-textsearch.html">https://www.postgresql.org/docs/current/datatype-textsearch.html</a>.
</div>
<div id="ref-openaiclip" class="csl-entry" role="listitem">
Radford, Alec, Jong Wook Kim, Luke Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. <span>“Learning Transferable Visual Models from Natural Language Supervision.”</span> <em>Proceedings of the International Conference on Machine Learning (ICML)</em>. <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a>.
</div>
<div id="ref-flask" class="csl-entry" role="listitem">
Ronacher, Armin. 2010. <span>“Flask: Web Development, One Drop at a Time.”</span> <a href="https://flask.palletsprojects.com/" class="uri">https://flask.palletsprojects.com/</a>.
</div>
<div id="ref-streamlit" class="csl-entry" role="listitem">
Streamlit Inc. 2019. <span>“Streamlit.”</span> <a href="https://streamlit.io" class="uri">https://streamlit.io</a>.
</div>
<div id="ref-Wang2020MiniLMDS" class="csl-entry" role="listitem">
Wang, Wenhui, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. <span>“MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers.”</span> <em>arXiv Preprint arXiv:2002.10957</em>. <a href="https://arxiv.org/abs/2002.10957">https://arxiv.org/abs/2002.10957</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/FinlyWealth\.github\.io\/mds-finly-search\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>