{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../src')\n",
    "from preprocess import initialize_clip_model, generate_embedding\n",
    "from retrieval import hybrid_retrieval, PostgresVectorRetrieval, TextSearchRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is mps\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                   Pid  \\\n",
       "0   230765.156074.66EA81FE0710E584.26DCA022C55D334...   \n",
       "1   230765.156074.66EA81FE0710E584.26DCA022C55D334...   \n",
       "2   230765.156074.66EA81FE0710E584.26DCA022C55D334...   \n",
       "3   178866.156074.820F1205554371C6.94435B3E5252BCD...   \n",
       "4   178866.156074.820F1205554371C6.94435B3E5252BCD...   \n",
       "5   178866.156074.820F1205554371C6.94435B3E5252BCD...   \n",
       "6   178866.156074.820F1205554371C6.BF3F547DC44EC64...   \n",
       "7   178866.156074.820F1205554371C6.BF3F547DC44EC64...   \n",
       "8   178866.156074.820F1205554371C6.BF3F547DC44EC64...   \n",
       "9   159390.1.5EDD.B3A17A41A080BDA.US-10292524WH-5-...   \n",
       "10  159390.1.5EDD.B3A17A41A080BDA.US-10292524WH-5-...   \n",
       "11  159390.1.5EDD.B3A17A41A080BDA.US-10292524WH-5-...   \n",
       "12  159390.1.5EDD.7568CF40E1FC370A.US-30284883TO-3...   \n",
       "13  159390.1.5EDD.7568CF40E1FC370A.US-30284883TO-3...   \n",
       "14  159390.1.5EDD.7568CF40E1FC370A.US-30284883TO-3...   \n",
       "15  230765.156074.9E8AD04DF5D4FE7B.3BC8878B87A0460...   \n",
       "16  230765.156074.9E8AD04DF5D4FE7B.3BC8878B87A0460...   \n",
       "17  230765.156074.9E8AD04DF5D4FE7B.3BC8878B87A0460...   \n",
       "18  230765.156074.9E8AD04DF5D4FE7B.CFF09BDB285F3A6...   \n",
       "19  230765.156074.9E8AD04DF5D4FE7B.CFF09BDB285F3A6...   \n",
       "20  230765.156074.9E8AD04DF5D4FE7B.CFF09BDB285F3A6...   \n",
       "21  228963.156074.5BD1DE98FA94FFB1.F403820EFB11A4B...   \n",
       "22  228963.156074.5BD1DE98FA94FFB1.F403820EFB11A4B...   \n",
       "23  228963.156074.5BD1DE98FA94FFB1.F403820EFB11A4B...   \n",
       "24  178866.156074.820F1205554371C6.51D5532837F7ABB...   \n",
       "25  178866.156074.820F1205554371C6.51D5532837F7ABB...   \n",
       "26  178866.156074.820F1205554371C6.51D5532837F7ABB...   \n",
       "27  162510.2.8FC07D5199D7103.92298BFDF63CF2B6.SG66...   \n",
       "28  162510.2.8FC07D5199D7103.92298BFDF63CF2B6.SG66...   \n",
       "29  162510.2.8FC07D5199D7103.92298BFDF63CF2B6.SG66...   \n",
       "\n",
       "                                                 Name  \\\n",
       "0                 TEMU 26pcs Silicone Daisy Beads Set   \n",
       "1                 TEMU 26pcs Silicone Daisy Beads Set   \n",
       "2                 TEMU 26pcs Silicone Daisy Beads Set   \n",
       "3   This Annoying Home Life: A Mindless Coloring B...   \n",
       "4   This Annoying Home Life: A Mindless Coloring B...   \n",
       "5   This Annoying Home Life: A Mindless Coloring B...   \n",
       "6                                     A February Face   \n",
       "7                                     A February Face   \n",
       "8                                     A February Face   \n",
       "9     Puma Man T-shirt Black Size M Cotton, Polyester   \n",
       "10    Puma Man T-shirt Black Size M Cotton, Polyester   \n",
       "11    Puma Man T-shirt Black Size M Cotton, Polyester   \n",
       "12  Cos Woman Pants Midnight blue Size 4 Organic c...   \n",
       "13  Cos Woman Pants Midnight blue Size 4 Organic c...   \n",
       "14  Cos Woman Pants Midnight blue Size 4 Organic c...   \n",
       "15          TEMU 20pcs Low Temperature Soldering Rods   \n",
       "16          TEMU 20pcs Low Temperature Soldering Rods   \n",
       "17          TEMU 20pcs Low Temperature Soldering Rods   \n",
       "18  TEMU Elegant Solid Color Women's Dress With In...   \n",
       "19  TEMU Elegant Solid Color Women's Dress With In...   \n",
       "20  TEMU Elegant Solid Color Women's Dress With In...   \n",
       "21  Iowa Hawkeyes Champion Women's Bella T-Shirt S...   \n",
       "22  Iowa Hawkeyes Champion Women's Bella T-Shirt S...   \n",
       "23  Iowa Hawkeyes Champion Women's Bella T-Shirt S...   \n",
       "24  Baking Day!: Ready-to-Read Pre-Level 1 (Daniel...   \n",
       "25  Baking Day!: Ready-to-Read Pre-Level 1 (Daniel...   \n",
       "26  Baking Day!: Ready-to-Read Pre-Level 1 (Daniel...   \n",
       "27  Chlo CH0225SK Asian Fit 002 Women's Sunglasses...   \n",
       "28  Chlo CH0225SK Asian Fit 002 Women's Sunglasses...   \n",
       "29  Chlo CH0225SK Asian Fit 002 Women's Sunglasses...   \n",
       "\n",
       "                                                Query  \n",
       "0                                silicone daisy beads  \n",
       "1   daisy bead making kit for keychains and bracelets  \n",
       "2   TEMU 26pcs silicone daisy crafting beads set f...  \n",
       "3                         annoying life coloring book  \n",
       "4              stress relief coloring book for adults  \n",
       "5   mindless adult coloring book for stress and an...  \n",
       "6                                a february face book  \n",
       "7                        poetic novel A February Face  \n",
       "8   A February Face coming-of-age fiction book wit...  \n",
       "9                                  black puma t-shirt  \n",
       "10             puma men's black cotton t-shirt size M  \n",
       "11  medium size Puma black men’s t-shirt in cotton...  \n",
       "12                            cos midnight blue pants  \n",
       "13   size 4 COS organic cotton blend women's trousers  \n",
       "14  COS women's fitted pants in midnight blue with...  \n",
       "15                               low temp solder rods  \n",
       "16  TEMU metal soldering rods for copper and aluminum  \n",
       "17  non-toxic low temp solder rods for DIY repair ...  \n",
       "18                                   cape shawl dress  \n",
       "19          pleated polyester women's dress with cape  \n",
       "20  elegant solid color women's dress with pleated...  \n",
       "21                                iowa hawkeyes shirt  \n",
       "22      women's champion t-shirt small white hawkeyes  \n",
       "23  small women's white Champion Bella t-shirt Iow...  \n",
       "24                            daniel tiger baking day  \n",
       "25                        baking day pre-level reader  \n",
       "26  Daniel Tiger's Neighborhood early reader Bakin...  \n",
       "27                     chloe tortoiseshell sunglasses  \n",
       "28        CH0225SK chloe women’s sunglasses asian fit  \n",
       "29  Chloe CH0225SK tortoiseshell women’s sunglasse...  >"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('benchmark.csv')\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIPProcessor:\n",
       " - image_processor: CLIPImageProcessor {\n",
       "   \"crop_size\": {\n",
       "     \"height\": 224,\n",
       "     \"width\": 224\n",
       "   },\n",
       "   \"do_center_crop\": true,\n",
       "   \"do_convert_rgb\": true,\n",
       "   \"do_normalize\": true,\n",
       "   \"do_rescale\": true,\n",
       "   \"do_resize\": true,\n",
       "   \"image_mean\": [\n",
       "     0.48145466,\n",
       "     0.4578275,\n",
       "     0.40821073\n",
       "   ],\n",
       "   \"image_processor_type\": \"CLIPImageProcessor\",\n",
       "   \"image_std\": [\n",
       "     0.26862954,\n",
       "     0.26130258,\n",
       "     0.27577711\n",
       "   ],\n",
       "   \"resample\": 3,\n",
       "   \"rescale_factor\": 0.00392156862745098,\n",
       "   \"size\": {\n",
       "     \"shortest_edge\": 224\n",
       "   }\n",
       " }\n",
       " \n",
       " - tokenizer: CLIPTokenizerFast(name_or_path='openai/clip-vit-base-patch32', vocab_size=49408, model_max_length=77, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       " \t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       " }\n",
       " )\n",
       " \n",
       " {\n",
       "   \"processor_class\": \"CLIPProcessor\"\n",
       " },\n",
       " CLIPModel(\n",
       "   (text_model): CLIPTextTransformer(\n",
       "     (embeddings): CLIPTextEmbeddings(\n",
       "       (token_embedding): Embedding(49408, 512)\n",
       "       (position_embedding): Embedding(77, 512)\n",
       "     )\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPSdpaAttention(\n",
       "             (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "             (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "             (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (vision_model): CLIPVisionTransformer(\n",
       "     (embeddings): CLIPVisionEmbeddings(\n",
       "       (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "       (position_embedding): Embedding(50, 768)\n",
       "     )\n",
       "     (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "     (encoder): CLIPEncoder(\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x CLIPEncoderLayer(\n",
       "           (self_attn): CLIPSdpaAttention(\n",
       "             (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "             (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "           (mlp): CLIPMLP(\n",
       "             (activation_fn): QuickGELUActivation()\n",
       "             (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "           )\n",
       "           (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "   (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       " ))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models to test\n",
    "clip_model = \"openai/clip-vit-base-patch32\"\n",
    "initialize_clip_model(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hybrid_retrieval() got an unexpected keyword argument 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m query_embedding = generate_embedding(query_text=query)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Run hybrid search for single query\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m pids, scores = \u001b[43mhybrid_retrieval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Check if the ground truth Pid is in the results\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[33m'\u001b[39m\u001b[33mPid\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m pids:\n",
      "\u001b[31mTypeError\u001b[39m: hybrid_retrieval() got an unexpected keyword argument 'query'"
     ]
    }
   ],
   "source": [
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    'dbname': os.getenv('PGDATABASE', 'finly'),\n",
    "    'user': os.getenv('PGUSER', 'postgres'),\n",
    "    'password': os.getenv('PGPASSWORD', 'postgres'),\n",
    "    'host': os.getenv('PGHOST', 'localhost'),\n",
    "    'port': os.getenv('PGPORT', '5432')\n",
    "}\n",
    "TOP_K = 5\n",
    "\n",
    "# Create components\n",
    "components = [\n",
    "    PostgresVectorRetrieval('text_embedding', DB_CONFIG),\n",
    "    PostgresVectorRetrieval('image_embedding', DB_CONFIG),\n",
    "    TextSearchRetrieval('ts_rank_cd', DB_CONFIG)\n",
    "]\n",
    "\n",
    "weights = [0.5, 0.3, 0.2]  # Must sum to 1\n",
    "\n",
    "# Initialize counters\n",
    "hits = 0\n",
    "total = len(df)\n",
    "\n",
    "# Process each query individually\n",
    "for i, row in df.iterrows():\n",
    "    query = row['Query']\n",
    "    query_embedding = generate_embedding(query_text=query)\n",
    "    \n",
    "    # Run hybrid search for single query\n",
    "    pids, scores = hybrid_retrieval(\n",
    "        query=query,\n",
    "        query_embedding=query_embedding,\n",
    "        components=components,\n",
    "        weights=weights,\n",
    "        top_k=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Check if the ground truth Pid is in the results\n",
    "    if row['Pid'] in pids:\n",
    "        hits += 1\n",
    "\n",
    "recall_at_k = hits / total\n",
    "print(f\"Recall@{TOP_K}: {recall_at_k:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 175990,
     "sourceId": 396802,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "finly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
