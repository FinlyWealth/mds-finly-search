{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jchang/miniforge3/envs/finly/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/merged_output_sample_100k.parquet')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Pid', 'Name', 'ShortDescription', 'Description', 'CategoryId',\n",
       "       'Category', 'ImageURL', 'Price', 'PriceCurrency', 'SalePrice',\n",
       "       'FinalPrice', 'Discount', 'isOnSale', 'IsInStock', 'Keywords', 'Brand',\n",
       "       'Manufacturer', 'MPN', 'UPCorEAN', 'SKU', 'Color', 'Gender', 'Size',\n",
       "       'Condition', 'AlternateImageUrl', 'AlternateImageUrl2',\n",
       "       'AlternateImageUrl3', 'AlternateImageUrl4', 'DeepLinkURL', 'LinkUrl'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 1000 samples for evaluation\n",
      "\n",
      "Testing: CLIP-512 — openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1c97ba577445c7a5055b75b4c3cf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CLIP-512:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: CLIP-768 — openai/clip-vit-large-patch14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e507d693e1ab466db7bba8768265cf9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CLIP-768:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: SigLIP-512 — google/siglip-base-patch16-512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26dbe6102df6428e93bd5cfa66c208fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SigLIP-512:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: SigLIP-1024 — google/siglip-so400m-patch14-384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c1454489be246deafcac371e9e2244f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing SigLIP-1024:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Results:\n",
      "             Accuracy (%)  Avg Similarity  Std Similarity  \\\n",
      "Model                                                       \n",
      "CLIP-512             88.8          0.1754          0.0644   \n",
      "CLIP-768             91.3          0.1097          0.0719   \n",
      "SigLIP-512           68.7         -0.0459          0.0706   \n",
      "SigLIP-1024          94.3         -0.0296          0.0675   \n",
      "\n",
      "             Correct Predictions  Total Predictions  \n",
      "Model                                                \n",
      "CLIP-512                     888               1000  \n",
      "CLIP-768                     913               1000  \n",
      "SigLIP-512                   687               1000  \n",
      "SigLIP-1024                  943               1000  \n"
     ]
    }
   ],
   "source": [
    "SAMPLE_SIZE = 1000\n",
    "\n",
    "if SAMPLE_SIZE < len(df):\n",
    "    df = df.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"Using {SAMPLE_SIZE} samples for evaluation\")\n",
    "else:\n",
    "    print(f\"Using all {len(df)} samples for evaluation\")\n",
    "\n",
    "# Models to test\n",
    "models = {\n",
    "    \"CLIP-512\": \"openai/clip-vit-base-patch32\",\n",
    "    \"CLIP-768\": \"openai/clip-vit-large-patch14\",\n",
    "    \"SigLIP-512\": \"google/siglip-base-patch16-512\",\n",
    "    \"SigLIP-1024\": \"google/siglip-so400m-patch14-384\"\n",
    "}\n",
    "\n",
    "model_results = {\n",
    "    'Model': [],\n",
    "    'Accuracy (%)': [],\n",
    "    'Avg Similarity': [],\n",
    "    'Std Similarity': [],\n",
    "    'Correct Predictions': [],\n",
    "    'Total Predictions': []\n",
    "\n",
    "}\n",
    "\n",
    "for name, model_id in models.items():\n",
    "    print(f\"\\nTesting: {name} — {model_id}\")\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    model = AutoModel.from_pretrained(model_id).to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    similarities_matrix = []\n",
    "    \n",
    "    batch_size = 10\n",
    "    num_batches = len(df) // batch_size + (1 if len(df) % batch_size != 0 else 0)\n",
    "    \n",
    "    for batch_start in tqdm(range(0, len(df), batch_size), total=num_batches, desc=f\"Processing {name}\"):\n",
    "        batch_end = min(batch_start + batch_size, len(df))\n",
    "        batch_df = df.iloc[batch_start:batch_end]\n",
    "        \n",
    "        # Process all texts in the batch\n",
    "        descriptions = batch_df['Name'].tolist()\n",
    "        text_inputs = processor(text=descriptions, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            text_embeddings = model.get_text_features(**text_inputs)\n",
    "            text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Process all images in the batch\n",
    "        images = []\n",
    "        for pid in batch_df['Pid']:\n",
    "            image_path = f\"../data/images/{pid}.jpeg\"\n",
    "            try:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "                images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {pid}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not images:\n",
    "            continue\n",
    "            \n",
    "        image_inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_embeddings = model.get_image_features(**image_inputs)\n",
    "            image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate similarity matrix for the batch\n",
    "        similarity_matrix = torch.mm(image_embeddings, text_embeddings.t())\n",
    "        \n",
    "        # For each image, find the most similar text\n",
    "        predicted_indices = torch.argmax(similarity_matrix, dim=1)\n",
    "        \n",
    "        # Count correct predictions (diagonal elements should be highest)\n",
    "        correct_predictions += (predicted_indices == torch.arange(len(images)).to(device)).sum().item()\n",
    "        total_predictions += len(images)\n",
    "        \n",
    "        # Store similarities for analysis\n",
    "        similarities_matrix.extend(similarity_matrix.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    similarities_array = np.array(similarities_matrix)\n",
    "    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0\n",
    "    \n",
    "    # Store results in the dictionary\n",
    "    model_results['Model'].append(name)\n",
    "    model_results['Accuracy (%)'].append(round(accuracy, 2))\n",
    "    model_results['Avg Similarity'].append(round(np.mean(similarities_array), 4))\n",
    "    model_results['Std Similarity'].append(round(np.std(similarities_array), 4))\n",
    "    model_results['Correct Predictions'].append(correct_predictions)\n",
    "    model_results['Total Predictions'].append(total_predictions)\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "results_df.set_index('Model', inplace=True)\n",
    "\n",
    "print(\"\\nModel Evaluation Results:\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 175990,
     "sourceId": 396802,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "finly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
